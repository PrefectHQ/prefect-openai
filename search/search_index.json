{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Coordinate and use AI in your dataflow with <code>prefect-openai</code>","text":"<p>The <code>prefect-openai</code> collection makes it easy to leverage the capabilities of AI in your flows. Check out the examples below to get started!</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#summarize-tracebacks-with-gpt3","title":"Summarize tracebacks with GPT3","text":"<p>Tracebacks--it's quintessential in programming. They are a record of every line of code leading to the error, to help us, humans, determine what's wrong with the program and find a solution.</p> <p>However, tracebacks can be extraordinarily complex, especially for someone new to the codebase.</p> <p>To streamline this process, we could add AI to the mix, to offer a more human-readable summary of the issue, so it's easier for the developer to understand what went wrong and implement a fix.</p> <p>After installing <code>prefect-openai</code> and saving an OpenAI key, you can easily incorporate OpenAI within your flows to help you achieve the aforementioned benefits!</p> <pre><code>from prefect import flow, get_run_logger\nfrom prefect_openai import OpenAICredentials, CompletionModel\n\n\n@flow\ndef summarize_traceback(traceback: str) -&gt; str:\n    logger = get_run_logger()\n    openai_credentials = OpenAICredentials.load(\"openai-credentials\")\n    completion_model = CompletionModel(\n        openai_credentials=openai_credentials,\n        model=\"text-curie-001\",\n        max_tokens=512,\n    )\n    prompt = f\"Summarize cause of error from this traceback: ```{traceback}```\"\n    summary = completion_model.submit_prompt(traceback).choices[0][\"text\"]\n    logger.info(f\"Summary of the traceback: {summary}\")\n    return summary\n\n\nif __name__ == \"__main__\":\n    traceback = \"\"\"\n        ParameterBindError: Error binding parameters for function 'summarize_traceback': missing a required argument: 'traceback'.\n        Function 'summarize_traceback' has signature 'traceback: str) -&gt; str' but received args: () and kwargs: {}.\n    \"\"\"\n    summarize_traceback(traceback)\n</code></pre> <p><pre><code>...\n12:29:32.085 | INFO    | Flow run 'analytic-starling' - Finished text completion using the 'text-curie-001' model with 113 tokens, creating 1 choice(s).\n12:29:32.089 | INFO    | Flow run 'analytic-starling' - Summary of the traceback:     \nThis error is caused by the missing argument traceback. The function expects a traceback object as its first argument, but received nothing.\n...\n</code></pre> Notice how the original traceback was quite long and confusing.</p> <p>On the flip side, the Curie GPT3 model was able to summarize the issue eloquently!</p> <p>Built-in decorator</p> <p>No need to build this yourself, <code>prefect-openai</code> features a built-in decorator to help you automatically catch and interpret exceptions in flows, tasks, and even vanilla Python functions.</p> <pre><code>import httpx\nfrom prefect_openai.completion import interpret_exception\n\n@interpret_exception(\"COMPLETION_MODEL_BLOCK_NAME_PLACEHOLDER\")\ndef example_func():\n    resp = httpx.get(\"https://httpbin.org/status/403\")\n    resp.raise_for_status()\n\nexample_func()\n</code></pre>"},{"location":"#create-a-story-around-a-flow-run-name-with-gpt3-and-dall-e","title":"Create a story around a flow run name with GPT3 and DALL-E","text":"<p>Have you marveled at all the AI-generated images and wondered how others did it?</p> <p>After installing <code>prefect-openai</code> and saving an OpenAI key, you, too, can create AI-generated art.</p> <p>Here's an example on how to create a story and an image based off a flow run name.</p> <pre><code>from prefect import flow, task, get_run_logger\nfrom prefect.context import get_run_context\nfrom prefect_openai import OpenAICredentials, ImageModel, CompletionModel\n\n\n@task\ndef create_story_from_name(credentials: OpenAICredentials, flow_run_name: str) -&gt; str:\n    \"\"\"\n    Create a fun story about the flow run name.\n    \"\"\"\n    text_model = CompletionModel(\n        openai_credentials=credentials, model=\"text-curie-001\", max_tokens=288\n    )\n    text_prompt = f\"Provide a fun story about a {flow_run_name}\"\n    story = text_model.submit_prompt(text_prompt).choices[0].text.strip()\n    return story\n\n\n@task\ndef create_image_from_story(credentials: OpenAICredentials, story: str) -&gt; str:\n    \"\"\"\n    Create an image associated with the story.\n    \"\"\"\n    image_model = ImageModel(openai_credentials=credentials, size=\"512x512\")\n    image_result = image_model.submit_prompt(story)\n    image_url = image_result.data[0][\"url\"]\n    return image_url\n\n\n@flow\ndef create_story_and_image_from_flow_run_name() -&gt; str:\n    \"\"\"\n    Get the flow run name and create a story and image associated with it.\n    \"\"\"\n    context = get_run_context()\n    flow_run_name = context.flow_run.name.replace(\"-\", \" \")\n\n    credentials = OpenAICredentials.load(\"openai-credentials\")\n    story = create_story_from_name(credentials=credentials, flow_run_name=flow_run_name)\n    image_url = create_image_from_story(credentials=credentials, story=story)\n\n    story_and_image = (\n        f\"The story about a {flow_run_name}: '{story}' \"\n        f\"And its image: {image_url}\"\n    )\n    print(story_and_image)\n    return story_and_image\n\n\ncreate_story_and_image_from_flow_run_name()\n</code></pre>"},{"location":"#saving-an-openai-key","title":"Saving an OpenAI key","text":"<p>It's easy to set up an <code>OpenAICredentials</code> block!</p> <ol> <li>Head over to https://beta.openai.com/account/api-keys</li> <li>Login to your OpenAI account</li> <li>Click \"+ Create new secret key\"</li> <li>Copy the generated API key</li> <li>Create a short script, replacing the placeholders (or do so in the UI)</li> </ol> <pre><code>from prefect_openai import OpenAICredentials`\nOpenAICredentials(api_key=\"API_KEY_PLACEHOLDER\").save(\"BLOCK_NAME_PLACEHOLDER\")\n</code></pre> <p>Congrats! You can now easily load the saved block, which holds your OpenAI API key:</p> <pre><code>from prefect_openai import OpenAICredentials\nOpenAICredentials.load(\"BLOCK_NAME_PLACEHOLDER\")\n</code></pre> <p>Visit Flow Run Name Art to see some example output!</p>"},{"location":"#resources","title":"Resources","text":"<p>For more tips on how to use tasks and flows in a Collection, check out Using Collections!</p>"},{"location":"#installation","title":"Installation","text":"<p>Install <code>prefect-openai</code> with <code>pip</code>:</p> <pre><code>pip install prefect-openai\n</code></pre> <p>Requires an installation of Python 3.7+.</p> <p>We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv.</p> <p>These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation.</p>"},{"location":"#feedback","title":"Feedback","text":"<p>If you encounter any bugs while using <code>prefect-openai</code>, feel free to open an issue in the prefect-openai repository.</p> <p>If you have any questions or issues while using <code>prefect-openai</code>, you can find help in either the Prefect Discourse forum or the Prefect Slack community.</p> <p>Feel free to star or watch <code>prefect-openai</code> for updates too!</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you'd like to help contribute to fix an issue or add a feature to <code>prefect-openai</code>, please propose changes through a pull request from a fork of the repository.</p> <p>Here are the steps:</p> <ol> <li>Fork the repository</li> <li>Clone the forked repository</li> <li>Install the repository and its dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Make desired changes</li> <li>Add tests</li> <li>Insert an entry to CHANGELOG.md</li> <li>Install <code>pre-commit</code> to perform quality checks prior to commit: <pre><code>pre-commit install\n</code></pre></li> <li><code>git commit</code>, <code>git push</code>, and create a pull request</li> </ol>"},{"location":"blocks_catalog/","title":"Blocks Catalog","text":"<p>Below is a list of Blocks available for registration in <code>prefect-openai</code>.</p> <p>To register blocks in this module to view and edit them on Prefect Cloud: <pre><code>prefect block register -m prefect_openai\n</code></pre> Note, to use the <code>load</code> method on Blocks, you must already have a block document saved through code or saved through the UI.</p>"},{"location":"blocks_catalog/#credentials-module","title":"Credentials Module","text":"<p>OpenAICredentials</p> <p>Credentials used to authenticate with OpenAI.</p> <p>To load the OpenAICredentials: <pre><code>from prefect import flow\nfrom prefect_openai.credentials import OpenAICredentials\n\n@flow\ndef my_flow():\n    my_block = OpenAICredentials.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Credentials Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#completion-module","title":"Completion Module","text":"<p>CompletionModel</p> <p>A block that contains config for an OpenAI Completion Model. Learn more in the OpenAPI Text Completion docs.</p> <p>To load the CompletionModel: <pre><code>from prefect import flow\nfrom prefect_openai.completion import CompletionModel\n\n@flow\ndef my_flow():\n    my_block = CompletionModel.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Completion Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#image-module","title":"Image Module","text":"<p>ImageModel</p> <p>A block that contains config for an OpenAI Image Model. Learn more in the OpenAPI Image generation docs.</p> <p>To load the ImageModel: <pre><code>from prefect import flow\nfrom prefect_openai.image import ImageModel\n\n@flow\ndef my_flow():\n    my_block = ImageModel.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Image Module under Examples Catalog.</p>"},{"location":"completion/","title":"Completion","text":""},{"location":"completion/#prefect_openai.completion","title":"<code>prefect_openai.completion</code>","text":"<p>Module for generating and configuring OpenAI completions.</p>"},{"location":"completion/#prefect_openai.completion-classes","title":"Classes","text":""},{"location":"completion/#prefect_openai.completion.CompletionModel","title":"<code>CompletionModel</code>","text":"<p>             Bases: <code>Block</code></p> <p>A block that contains config for an OpenAI Completion Model. Learn more in the OpenAPI Text Completion docs</p> <p>Attributes:</p> Name Type Description <code>openai_credentials</code> <code>OpenAICredentials</code> <p>The credentials used to authenticate with OpenAI.</p> <code>model</code> <code>Union[Literal['text-davinci-003', 'text-curie-001', 'text-babbage-001', 'text-ada-001'], str]</code> <p>ID of the model to use.</p> <code>temperature</code> <code>float</code> <p>What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.</p> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).</p> <code>suffix</code> <code>Optional[str]</code> <p>The suffix to append to the prompt.</p> <code>echo</code> <code>bool</code> <p>Echo back the prompt in addition to the completion.</p> <code>timeout</code> <code>Optional[float]</code> <p>The maximum time to wait for the model to warm up.</p> Example <p>Load a configured block: <pre><code>from prefect_openai import CompletionModel\n\ncompletion_model = CompletionModel.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect_openai/completion.py</code> <pre><code>class CompletionModel(Block):\n    \"\"\"\n    A block that contains config for an OpenAI Completion Model.\n    Learn more in the [OpenAPI Text Completion docs](\n        https://beta.openai.com/docs/guides/completion)\n\n    Attributes:\n        openai_credentials: The credentials used to authenticate with OpenAI.\n        model: ID of the model to use.\n        temperature: What sampling temperature to use.\n            Higher values means the model will take more risks.\n            Try 0.9 for more creative applications, and 0 (argmax sampling)\n            for ones with a well-defined answer.\n        max_tokens: The maximum number of tokens to generate in the completion.\n            The token count of your prompt plus max_tokens cannot exceed the\n            model's context length. Most models have a context length of 2048 tokens\n            (except for the newest models, which support 4096).\n        suffix: The suffix to append to the prompt.\n        echo: Echo back the prompt in addition to the completion.\n        timeout: The maximum time to wait for the model to warm up.\n\n    Example:\n        Load a configured block:\n        ```python\n        from prefect_openai import CompletionModel\n\n        completion_model = CompletionModel.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    openai_credentials: OpenAICredentials = Field(\n        default=..., description=\"The credentials used to authenticate with OpenAI.\"\n    )\n    model: Union[\n        Literal[\n            \"text-davinci-003\", \"text-curie-001\", \"text-babbage-001\", \"text-ada-001\"\n        ],\n        str,\n    ] = Field(default=\"text-curie-001\", description=\"ID of the model to use.\")\n    temperature: float = Field(\n        default=0.5,\n        description=(\n            \"What sampling temperature to use. Higher values means the model will take \"\n            \"more risks. Try 0.9 for more creative applications, and 0 \"\n            \"(argmax sampling) for ones with a well-defined answer.\"\n        ),\n    )\n    max_tokens: int = Field(\n        default=16,\n        description=(\n            \"The maximum number of tokens to generate in the completion. \"\n            \"The token count of your prompt plus max_tokens cannot exceed the \"\n            \"model's context length. Most models have a context length of 2048 tokens \"\n            \"(except for the newest models, which support 4096).\"\n        ),\n    )\n    suffix: Optional[str] = Field(\n        default=None, description=\"The suffix to append to the prompt.\"\n    )\n    echo: bool = Field(default=False, description=\"Whether to echo the prompt.\")\n    timeout: Optional[float] = Field(\n        default=None, description=\"The maximum time to wait for the model to warm up.\"\n    )\n\n    _block_type_name = \"OpenAI Completion Model\"\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/760539393a7dbf93a143fb01c2a8b0fe7157a8d8-247x250.png\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-openai/completion/#prefect_openai.completion.CompletionModel\"  # noqa\n\n    @property\n    def logger(self) -&gt; Logger:\n        \"\"\"\n        Returns a logger based on whether the CompletionModel\n        is called from within a flow or task run context.\n        If a run context is present, the logger property returns a run logger.\n        Else, it returns a default logger labeled with the class's name.\n\n        Returns:\n            The run logger or a default logger with the class's name.\n        \"\"\"\n        try:\n            return get_run_logger()\n        except MissingContextError:\n            return get_logger(self.__class__.__name__)\n\n    @sync_compatible\n    async def submit_prompt(\n        self, prompt: str, **acreate_kwargs: Dict[str, Any]\n    ) -&gt; OpenAIObject:\n        \"\"\"\n        Submits a prompt for the model to generate a text completion.\n        OpenAI will return an object potentially containing multiple `choices`,\n        where the zeroth index is what they consider the \"best\" completion.\n        Learn more in the [OpenAPI Text Completion docs](\n            https://beta.openai.com/docs/guides/completion)\n\n        Args:\n            prompt: The prompt to use for the completion.\n            **acreate_kwargs: Additional keyword arguments to pass\n                to [`openai.Completion.acreate`](\n                https://beta.openai.com/docs/api-reference/completions/create).\n\n        Returns:\n            The OpenAIObject containing the completion and associated metadata.\n\n        Example:\n            Create an OpenAI Completion given a prompt:\n            ```python\n            from prefect import flow\n            from prefect_openai import CompletionModel, OpenAICredentials\n\n            @flow(log_prints=True)\n            def my_ai_bot(model_name: str = \"text-davinci-003\")\n                credentials = OpenAICredentials.load(\"my-openai-creds\")\n\n                completion_model = CompletionModel(\n                    openai_credentials=credentials,\n                )\n\n                for prompt in [\"hi!\", \"what is the meaning of life?\"]:\n                    completion = completion_model.submit_prompt(prompt)\n                    print(completion.choices[0].text)\n            ```\n        \"\"\"\n        client = self.openai_credentials.get_client()\n\n        input_kwargs = dict(\n            model=self.model,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            suffix=self.suffix,\n            echo=self.echo,\n            timeout=self.timeout,\n        )\n        input_kwargs.update(acreate_kwargs)\n\n        creation = await client.Completion.acreate(prompt=prompt, **input_kwargs)\n        total_tokens = creation.usage[\"total_tokens\"]\n        num_choices = len(creation.choices)\n        self.logger.info(\n            f\"Finished text completion using the {self.model!r} \"\n            f\"model with {total_tokens} tokens, creating {num_choices} choice(s).\"\n        )\n        return creation\n</code></pre>"},{"location":"completion/#prefect_openai.completion.CompletionModel-attributes","title":"Attributes","text":""},{"location":"completion/#prefect_openai.completion.CompletionModel.logger","title":"<code>logger: Logger</code>  <code>property</code>","text":"<p>Returns a logger based on whether the CompletionModel is called from within a flow or task run context. If a run context is present, the logger property returns a run logger. Else, it returns a default logger labeled with the class's name.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>The run logger or a default logger with the class's name.</p>"},{"location":"completion/#prefect_openai.completion.CompletionModel-functions","title":"Functions","text":""},{"location":"completion/#prefect_openai.completion.CompletionModel.submit_prompt","title":"<code>submit_prompt</code>  <code>async</code>","text":"<p>Submits a prompt for the model to generate a text completion. OpenAI will return an object potentially containing multiple <code>choices</code>, where the zeroth index is what they consider the \"best\" completion. Learn more in the OpenAPI Text Completion docs</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to use for the completion.</p> required <code>**acreate_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>openai.Completion.acreate</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAIObject</code> <p>The OpenAIObject containing the completion and associated metadata.</p> Example <p>Create an OpenAI Completion given a prompt: <pre><code>from prefect import flow\nfrom prefect_openai import CompletionModel, OpenAICredentials\n\n@flow(log_prints=True)\ndef my_ai_bot(model_name: str = \"text-davinci-003\")\n    credentials = OpenAICredentials.load(\"my-openai-creds\")\n\n    completion_model = CompletionModel(\n        openai_credentials=credentials,\n    )\n\n    for prompt in [\"hi!\", \"what is the meaning of life?\"]:\n        completion = completion_model.submit_prompt(prompt)\n        print(completion.choices[0].text)\n</code></pre></p> Source code in <code>prefect_openai/completion.py</code> <pre><code>@sync_compatible\nasync def submit_prompt(\n    self, prompt: str, **acreate_kwargs: Dict[str, Any]\n) -&gt; OpenAIObject:\n    \"\"\"\n    Submits a prompt for the model to generate a text completion.\n    OpenAI will return an object potentially containing multiple `choices`,\n    where the zeroth index is what they consider the \"best\" completion.\n    Learn more in the [OpenAPI Text Completion docs](\n        https://beta.openai.com/docs/guides/completion)\n\n    Args:\n        prompt: The prompt to use for the completion.\n        **acreate_kwargs: Additional keyword arguments to pass\n            to [`openai.Completion.acreate`](\n            https://beta.openai.com/docs/api-reference/completions/create).\n\n    Returns:\n        The OpenAIObject containing the completion and associated metadata.\n\n    Example:\n        Create an OpenAI Completion given a prompt:\n        ```python\n        from prefect import flow\n        from prefect_openai import CompletionModel, OpenAICredentials\n\n        @flow(log_prints=True)\n        def my_ai_bot(model_name: str = \"text-davinci-003\")\n            credentials = OpenAICredentials.load(\"my-openai-creds\")\n\n            completion_model = CompletionModel(\n                openai_credentials=credentials,\n            )\n\n            for prompt in [\"hi!\", \"what is the meaning of life?\"]:\n                completion = completion_model.submit_prompt(prompt)\n                print(completion.choices[0].text)\n        ```\n    \"\"\"\n    client = self.openai_credentials.get_client()\n\n    input_kwargs = dict(\n        model=self.model,\n        temperature=self.temperature,\n        max_tokens=self.max_tokens,\n        suffix=self.suffix,\n        echo=self.echo,\n        timeout=self.timeout,\n    )\n    input_kwargs.update(acreate_kwargs)\n\n    creation = await client.Completion.acreate(prompt=prompt, **input_kwargs)\n    total_tokens = creation.usage[\"total_tokens\"]\n    num_choices = len(creation.choices)\n    self.logger.info(\n        f\"Finished text completion using the {self.model!r} \"\n        f\"model with {total_tokens} tokens, creating {num_choices} choice(s).\"\n    )\n    return creation\n</code></pre>"},{"location":"completion/#prefect_openai.completion-functions","title":"Functions","text":""},{"location":"completion/#prefect_openai.completion.interpret_exception","title":"<code>interpret_exception</code>","text":"<p>Use OpenAI to interpret the exception raised from the decorated function. If used with a flow and return_state=True, will override the original state's data and message with the OpenAI interpretation.</p> <p>Parameters:</p> Name Type Description Default <code>completion_model_name</code> <code>str</code> <p>The name of the CompletionModel block to use to interpret the caught exception.</p> required <code>prompt_prefix</code> <code>str</code> <p>The prefix to include in the prompt ahead of the traceback and exception message.</p> <code>'Explain:'</code> <code>traceback_tail</code> <code>int</code> <p>The number of lines of the original traceback to include in the prompt to OpenAI, starting from the tail. If 0, only include the exception message in the prompt. Note this can be costly in terms of tokens so be sure to set this and the max_tokens in CompletionModel appropriately.</p> <code>0</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A decorator that will use an OpenAI CompletionModel to interpret the exception raised from the decorated function.</p> <p>Examples:</p> <p>Interpret the exception raised from a flow. <pre><code>import httpx\nfrom prefect import flow\nfrom prefect_openai.completion import interpret_exception\n\n@flow\n@interpret_exception(\"COMPLETION_MODEL_BLOCK_NAME_PLACEHOLDER\")\ndef example_flow():\n    resp = httpx.get(\"https://httpbin.org/status/403\")\n    resp.raise_for_status()\n\nexample_flow()\n</code></pre></p> <p>Use a unique prefix and include the last line of the traceback in the prompt. <pre><code>import httpx\nfrom prefect import flow\nfrom prefect_openai.completion import interpret_exception\n\n@flow\n@interpret_exception(\n    \"COMPLETION_MODEL_BLOCK_NAME_PLACEHOLDER\",\n    prompt_prefix=\"Offer a solution:\",\n    traceback_tail=1,\n)\ndef example_flow():\n    resp = httpx.get(\"https://httpbin.org/status/403\")\n    resp.raise_for_status()\n\nexample_flow()\n</code></pre></p> Source code in <code>prefect_openai/completion.py</code> <pre><code>def interpret_exception(\n    completion_model_name: str, prompt_prefix: str = \"Explain:\", traceback_tail: int = 0\n) -&gt; Callable:\n    \"\"\"\n    Use OpenAI to interpret the exception raised from the decorated function.\n    If used with a flow and return_state=True, will override the original state's\n    data and message with the OpenAI interpretation.\n\n    Args:\n        completion_model_name: The name of the CompletionModel\n            block to use to interpret the caught exception.\n        prompt_prefix: The prefix to include in the prompt ahead of the traceback\n            and exception message.\n        traceback_tail: The number of lines of the original traceback to include\n            in the prompt to OpenAI, starting from the tail. If 0, only include the\n            exception message in the prompt. Note this can be costly in terms of tokens\n            so be sure to set this and the max_tokens in CompletionModel appropriately.\n\n    Returns:\n        A decorator that will use an OpenAI CompletionModel to interpret the exception\n            raised from the decorated function.\n\n    Examples:\n        Interpret the exception raised from a flow.\n        ```python\n        import httpx\n        from prefect import flow\n        from prefect_openai.completion import interpret_exception\n\n        @flow\n        @interpret_exception(\"COMPLETION_MODEL_BLOCK_NAME_PLACEHOLDER\")\n        def example_flow():\n            resp = httpx.get(\"https://httpbin.org/status/403\")\n            resp.raise_for_status()\n\n        example_flow()\n        ```\n\n        Use a unique prefix and include the last line of the traceback in the prompt.\n        ```python\n        import httpx\n        from prefect import flow\n        from prefect_openai.completion import interpret_exception\n\n        @flow\n        @interpret_exception(\n            \"COMPLETION_MODEL_BLOCK_NAME_PLACEHOLDER\",\n            prompt_prefix=\"Offer a solution:\",\n            traceback_tail=1,\n        )\n        def example_flow():\n            resp = httpx.get(\"https://httpbin.org/status/403\")\n            resp.raise_for_status()\n\n        example_flow()\n        ```\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; Callable:\n        \"\"\"\n        The actual decorator.\n        \"\"\"\n        if isinstance(fn, (Flow, Task)):\n            raise ValueError(\n                \"interpret_exception should be nested under the flow / task decorator, \"\n                \"e.g. `@flow` -&gt; `@interpret_exception('curie')` -&gt; `def function()`\"\n            )\n\n        @functools.wraps(fn)\n        def sync_wrapper(*args: Tuple[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n            \"\"\"\n            The sync version of the wrapper function that will execute the function.\n            \"\"\"\n            try:\n                return fn(*args, **kwargs)\n            except Exception as exc:\n                _raise_interpreted_exc(\n                    completion_model_name, prompt_prefix, traceback_tail, exc\n                )\n\n        # couldn't get sync_compatible working so had to define an async flavor\n        @functools.wraps(fn)\n        async def async_wrapper(*args: Tuple[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n            \"\"\"\n            The async version of the wrapper function that will execute the function.\n            \"\"\"\n            try:\n                return await fn(*args, **kwargs)\n            except Exception as exc:\n                await _raise_interpreted_exc(\n                    completion_model_name, prompt_prefix, traceback_tail, exc\n                )\n\n        wrapper = async_wrapper if is_async_fn(fn) else sync_wrapper\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"credentials/","title":"Credentials","text":""},{"location":"credentials/#prefect_openai.credentials","title":"<code>prefect_openai.credentials</code>","text":"<p>Module for authenticating with OpenAI.</p>"},{"location":"credentials/#prefect_openai.credentials-classes","title":"Classes","text":""},{"location":"credentials/#prefect_openai.credentials.OpenAICredentials","title":"<code>OpenAICredentials</code>","text":"<p>             Bases: <code>CredentialsBlock</code></p> <p>Credentials used to authenticate with OpenAI.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>SecretStr</code> <p>The API key used to authenticate with OpenAI.</p> Example <p>Load a configured block: <pre><code>from prefect_openai import OpenAICredentials\n\ncredentials = OpenAICredentials.load(\"BLOCK_NAME\")\n</code></pre></p> <p>Get the OpenAPI client: <pre><code>from prefect_openai import OpenAICredentials\n\ncredentials = OpenAICredentials.load(\"BLOCK_NAME\")\nclient = credentials.get_client()\n</code></pre></p> Source code in <code>prefect_openai/credentials.py</code> <pre><code>class OpenAICredentials(CredentialsBlock):\n    \"\"\"\n    Credentials used to authenticate with OpenAI.\n\n    Attributes:\n        api_key: The API key used to authenticate with OpenAI.\n\n    Example:\n        Load a configured block:\n        ```python\n        from prefect_openai import OpenAICredentials\n\n        credentials = OpenAICredentials.load(\"BLOCK_NAME\")\n        ```\n\n        Get the OpenAPI client:\n        ```python\n        from prefect_openai import OpenAICredentials\n\n        credentials = OpenAICredentials.load(\"BLOCK_NAME\")\n        client = credentials.get_client()\n        ```\n    \"\"\"\n\n    _block_type_name = \"OpenAI Credentials\"\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/760539393a7dbf93a143fb01c2a8b0fe7157a8d8-247x250.png\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-openai/credentials/#prefect_openai.credentials.OpenAICredentials\"  # noqa\n\n    api_key: SecretStr = Field(\n        default=...,\n        title=\"API Key\",\n        description=\"The API key used to authenticate with OpenAI.\",\n    )\n\n    organization: Optional[str] = Field(\n        default=None,\n        title=\"Organization\",\n        description=\"Specify which organization is used for an API request.\",\n    )\n\n    def get_client(self) -&gt; ModuleType:\n        \"\"\"\n        Gets the OpenAPI client.\n\n        Returns:\n            The OpenAPI client.\n        \"\"\"\n        openai.api_key = self.api_key.get_secret_value()\n        openai.organization = self.organization\n        return openai\n</code></pre>"},{"location":"credentials/#prefect_openai.credentials.OpenAICredentials-functions","title":"Functions","text":""},{"location":"credentials/#prefect_openai.credentials.OpenAICredentials.get_client","title":"<code>get_client</code>","text":"<p>Gets the OpenAPI client.</p> <p>Returns:</p> Type Description <code>ModuleType</code> <p>The OpenAPI client.</p> Source code in <code>prefect_openai/credentials.py</code> <pre><code>def get_client(self) -&gt; ModuleType:\n    \"\"\"\n    Gets the OpenAPI client.\n\n    Returns:\n        The OpenAPI client.\n    \"\"\"\n    openai.api_key = self.api_key.get_secret_value()\n    openai.organization = self.organization\n    return openai\n</code></pre>"},{"location":"examples_catalog/","title":"Examples Catalog","text":"<p>Below is a list of examples for <code>prefect-openai</code>.</p>"},{"location":"examples_catalog/#completion-module","title":"Completion Module","text":"<p>Create an OpenAI Completion given a prompt: <pre><code>from prefect import flow\nfrom prefect_openai import CompletionModel, OpenAICredentials\n\n@flow(log_prints=True)\ndef my_ai_bot(model_name: str = \"text-davinci-003\")\n    credentials = OpenAICredentials.load(\"my-openai-creds\")\n\n    completion_model = CompletionModel(\n        openai_credentials=credentials,\n    )\n\n    for prompt in [\"hi!\", \"what is the meaning of life?\"]:\n        completion = completion_model.submit_prompt(prompt)\n        print(completion.choices[0].text)\n</code></pre> Interpret the exception raised from a flow. <pre><code>import httpx\nfrom prefect import flow\nfrom prefect_openai.completion import interpret_exception\n\n@flow\n@interpret_exception(\"COMPLETION_MODEL_BLOCK_NAME_PLACEHOLDER\")\ndef example_flow():\n    resp = httpx.get(\"https://httpbin.org/status/403\")\n    resp.raise_for_status()\n\nexample_flow()\n</code></pre></p> <p>Use a unique prefix and include the last line of the traceback in the prompt. <pre><code>import httpx\nfrom prefect import flow\nfrom prefect_openai.completion import interpret_exception\n\n@flow\n@interpret_exception(\n    \"COMPLETION_MODEL_BLOCK_NAME_PLACEHOLDER\",\n    prompt_prefix=\"Offer a solution:\",\n    traceback_tail=1,\n)\ndef example_flow():\n    resp = httpx.get(\"https://httpbin.org/status/403\")\n    resp.raise_for_status()\n\nexample_flow()\n</code></pre></p>"},{"location":"examples_catalog/#credentials-module","title":"Credentials Module","text":"<p>Load a configured block: <pre><code>from prefect_openai import OpenAICredentials\n\ncredentials = OpenAICredentials.load(\"BLOCK_NAME\")\n</code></pre></p> <p>Get the OpenAPI client: <pre><code>from prefect_openai import OpenAICredentials\n\ncredentials = OpenAICredentials.load(\"BLOCK_NAME\")\nclient = credentials.get_client()\n</code></pre></p>"},{"location":"examples_catalog/#image-module","title":"Image Module","text":"<p>Create an OpenAI Image given a prompt: <pre><code>from prefect_openai import ImageModel\n\nimage_model = ImageModel.load(\"BLOCK_NAME\")\nimage_model.submit_prompt(prompt=\"A prompt for an image.\")\n</code></pre></p>"},{"location":"flow_run_name_art/","title":"Flow Run Name Art","text":"<p>Below are some fun stories and images based off flow run names, created using <code>prefect-openai</code>, GPT3, and DALL-E.</p> <p>To create one, you can follow this tutorial here!</p> <p>If you'd like to share, please feel free to make a pull request:</p> <ul> <li>Create a new heading with the flow run name</li> <li>Copy and paste the story</li> <li>Copy and paste the image</li> </ul> <p>We'd love to see it!</p>"},{"location":"flow_run_name_art/#malachite-mule","title":"Malachite Mule","text":"<p>Malachite is a mule that was born with a unique ability. She can change her color at will, and she loves to play with her color changing friends.</p> <p></p>"},{"location":"flow_run_name_art/#witty-jaybird","title":"Witty Jaybird","text":"<p>Jaybird was always one to think on his feet. He could come up with the funniest jokes and say the smartest things. He was always the life of the party. But sometimes, Jaybird's quick wit can get him into trouble.</p> <p></p>"},{"location":"image/","title":"Image","text":""},{"location":"image/#prefect_openai.image","title":"<code>prefect_openai.image</code>","text":"<p>Module for generating and configuring OpenAI images.</p>"},{"location":"image/#prefect_openai.image-classes","title":"Classes","text":""},{"location":"image/#prefect_openai.image.ImageModel","title":"<code>ImageModel</code>","text":"<p>             Bases: <code>Block</code></p> <p>A block that contains config for an OpenAI Image Model. Learn more in the OpenAPI Image generation docs</p> <p>Attributes:</p> Name Type Description <code>openai_credentials</code> <code>OpenAICredentials</code> <p>The credentials used to authenticate with OpenAI.</p> <code>size</code> <code>Literal['256x256', '512x512', '1024x1024']</code> <p>The size of the image to generate.</p> <code>n</code> <code>int</code> <p>The number of images to generate.</p> <code>response_format</code> <code>Literal['url', 'b64_json']</code> <p>The format of the image to generate.</p> Example <p>Load a configured block: <pre><code>from prefect_openai import ImageModel\n\nimage_model = ImageModel.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect_openai/image.py</code> <pre><code>class ImageModel(Block):\n    \"\"\"\n    A block that contains config for an OpenAI Image Model.\n    Learn more in the [OpenAPI Image generation docs](\n        https://beta.openai.com/docs/guides/images)\n\n    Attributes:\n        openai_credentials: The credentials used to authenticate with OpenAI.\n        size: The size of the image to generate.\n        n: The number of images to generate.\n        response_format: The format of the image to generate.\n\n    Example:\n        Load a configured block:\n        ```python\n        from prefect_openai import ImageModel\n\n        image_model = ImageModel.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    openai_credentials: OpenAICredentials = Field(\n        default=..., description=\"The credentials used to authenticate with OpenAI.\"\n    )\n    size: Literal[\"256x256\", \"512x512\", \"1024x1024\"] = Field(\n        default=\"256x256\", description=\"The size of the image to generate.\"\n    )\n    n: int = Field(\n        default=1,\n        title=\"Number of images\",\n        description=\"The number of images to generate.\",\n    )\n    response_format: Literal[\"url\", \"b64_json\"] = Field(\n        default=\"url\", description=\"The format of the image to generate.\"\n    )\n\n    _block_type_name = \"OpenAI Image Model\"\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/760539393a7dbf93a143fb01c2a8b0fe7157a8d8-247x250.png\"  # noqa\n\n    @property\n    def logger(self) -&gt; Logger:\n        \"\"\"\n        Returns a logger based on whether the ImageModel\n        is called from within a flow or task run context.\n        If a run context is present, the logger property returns a run logger.\n        Else, it returns a default logger labeled with the class's name.\n\n        Returns:\n            The run logger or a default logger with the class's name.\n        \"\"\"\n        try:\n            return get_run_logger()\n        except MissingContextError:\n            return get_logger(self.__class__.__name__)\n\n    @sync_compatible\n    async def submit_prompt(\n        self, prompt: str, **acreate_kwargs: Dict[str, Any]\n    ) -&gt; OpenAIObject:\n        \"\"\"\n        Submits a prompt for the model to generate an image.\n        Learn more in the [OpenAPI Image generation docs](\n            https://beta.openai.com/docs/guides/images)\n\n        Args:\n            prompt: The prompt to use for the image.\n            **acreate_kwargs: Additional keyword arguments to pass\n                to [`openai.Image.acreate`](\n                https://beta.openai.com/docs/api-reference/images/create).\n\n        Returns:\n            The OpenAIObject containing the image and associated metadata.\n\n        Example:\n            Create an OpenAI Image given a prompt:\n            ```python\n            from prefect_openai import ImageModel\n\n            image_model = ImageModel.load(\"BLOCK_NAME\")\n            image_model.submit_prompt(prompt=\"A prompt for an image.\")\n            ```\n        \"\"\"\n        client = self.openai_credentials.get_client()\n\n        input_kwargs = dict(\n            size=self.size,\n            n=self.n,\n            response_format=self.response_format,\n        )\n        input_kwargs.update(acreate_kwargs)\n        creation = await client.Image.acreate(prompt=prompt, **input_kwargs)\n        self.logger.info(\n            f\"Finished image completion, creating \" f\"{self.n} {self.size!r} image(s).\"\n        )\n        return creation\n</code></pre>"},{"location":"image/#prefect_openai.image.ImageModel-attributes","title":"Attributes","text":""},{"location":"image/#prefect_openai.image.ImageModel.logger","title":"<code>logger: Logger</code>  <code>property</code>","text":"<p>Returns a logger based on whether the ImageModel is called from within a flow or task run context. If a run context is present, the logger property returns a run logger. Else, it returns a default logger labeled with the class's name.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>The run logger or a default logger with the class's name.</p>"},{"location":"image/#prefect_openai.image.ImageModel-functions","title":"Functions","text":""},{"location":"image/#prefect_openai.image.ImageModel.submit_prompt","title":"<code>submit_prompt</code>  <code>async</code>","text":"<p>Submits a prompt for the model to generate an image. Learn more in the OpenAPI Image generation docs</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to use for the image.</p> required <code>**acreate_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>openai.Image.acreate</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAIObject</code> <p>The OpenAIObject containing the image and associated metadata.</p> Example <p>Create an OpenAI Image given a prompt: <pre><code>from prefect_openai import ImageModel\n\nimage_model = ImageModel.load(\"BLOCK_NAME\")\nimage_model.submit_prompt(prompt=\"A prompt for an image.\")\n</code></pre></p> Source code in <code>prefect_openai/image.py</code> <pre><code>@sync_compatible\nasync def submit_prompt(\n    self, prompt: str, **acreate_kwargs: Dict[str, Any]\n) -&gt; OpenAIObject:\n    \"\"\"\n    Submits a prompt for the model to generate an image.\n    Learn more in the [OpenAPI Image generation docs](\n        https://beta.openai.com/docs/guides/images)\n\n    Args:\n        prompt: The prompt to use for the image.\n        **acreate_kwargs: Additional keyword arguments to pass\n            to [`openai.Image.acreate`](\n            https://beta.openai.com/docs/api-reference/images/create).\n\n    Returns:\n        The OpenAIObject containing the image and associated metadata.\n\n    Example:\n        Create an OpenAI Image given a prompt:\n        ```python\n        from prefect_openai import ImageModel\n\n        image_model = ImageModel.load(\"BLOCK_NAME\")\n        image_model.submit_prompt(prompt=\"A prompt for an image.\")\n        ```\n    \"\"\"\n    client = self.openai_credentials.get_client()\n\n    input_kwargs = dict(\n        size=self.size,\n        n=self.n,\n        response_format=self.response_format,\n    )\n    input_kwargs.update(acreate_kwargs)\n    creation = await client.Image.acreate(prompt=prompt, **input_kwargs)\n    self.logger.info(\n        f\"Finished image completion, creating \" f\"{self.n} {self.size!r} image(s).\"\n    )\n    return creation\n</code></pre>"}]}